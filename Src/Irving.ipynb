{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT1244 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import heapq as heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bert Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Bert Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode sentences\n",
    "sentence1 = \"This is an example sentence with change negative.\"\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "\n",
    "sentence2 = \"This is an example sentence with change upset.\"\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "sentence3 = \"This is an example sentence with change happy.\"\n",
    "inputs3 = tokenizer(sentence3, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "    outputs3 = model(**inputs3)\n",
    "\n",
    "# Extract [CLS] token embedding (sentence-level representation)\n",
    "Vector_X = outputs1.last_hidden_state[:, 0, :]\n",
    "Vector_Y = outputs2.last_hidden_state[:, 0, :]\n",
    "Vector_Z = outputs3.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878990054130554"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(cosine_similarity(Vector_X, Vector_Y)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982489287853241"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(cosine_similarity(Vector_X, Vector_Z)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    return output.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the closer concepts are have higher cosine similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  sentiment_confidence  \\\n",
       "0           neutral                1.0000   \n",
       "1          positive                0.3486   \n",
       "2           neutral                0.6837   \n",
       "3          negative                1.0000   \n",
       "4          negative                1.0000   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"../Data/Raw/Tweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14639, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica plus you've added commercials to the experience... tacky.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_filter(text):\n",
    "    return re.sub(r\"@\\w+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(text):\n",
    "    return re.sub(r'[^A-Za-z ]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: at_filter(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: alpha_filter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>What  said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>thank you we got on a different flight to Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>leaving over  minutes Late Flight No warnings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Please bring American Airlines to BlackBerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>you have my money you change my flight and do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>we have  ppl so we need  know how many seats ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14639 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment  sentiment_confidence  \\\n",
       "0               neutral                1.0000   \n",
       "1              positive                0.3486   \n",
       "2               neutral                0.6837   \n",
       "3              negative                1.0000   \n",
       "4              negative                1.0000   \n",
       "...                 ...                   ...   \n",
       "14634          positive                0.3487   \n",
       "14635          negative                1.0000   \n",
       "14636           neutral                1.0000   \n",
       "14637          negative                1.0000   \n",
       "14638           neutral                0.6771   \n",
       "\n",
       "                                                    text  \n",
       "0                                             What  said  \n",
       "1       plus youve added commercials to the experienc...  \n",
       "2       I didnt today Must mean I need to take anothe...  \n",
       "3       its really aggressive to blast obnoxious ente...  \n",
       "4                and its a really big bad thing about it  \n",
       "...                                                  ...  \n",
       "14634   thank you we got on a different flight to Chi...  \n",
       "14635   leaving over  minutes Late Flight No warnings...  \n",
       "14636       Please bring American Airlines to BlackBerry  \n",
       "14637   you have my money you change my flight and do...  \n",
       "14638   we have  ppl so we need  know how many seats ...  \n",
       "\n",
       "[14639 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets[\"text\"], tweets[\"airline_sentiment\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"../Data/Cleaned/CleanTweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(map(lambda x: len(x), tweets[\"text\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = tweets[\"text\"].apply(lambda x: bert_encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(map(lambda x: x[0], sentence_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Data as a CSV so don't need to re-encode every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(data)\n",
    "#data_frame.to_txt(\"../Data/Cleaned/BERT_Vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, \"../Data/Cleaned/BERT_Vectors.oarquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_Encoded_Text = pd.read_parquet(\"../Data/Cleaned/BERT_Vectors.oarquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Bert_Encoded_Text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06095307,  0.08171247, -0.3694084 , ..., -0.12956254,\n",
       "         0.2612489 ,  0.22729596],\n",
       "       [ 0.05729439,  0.32379445, -0.21796885, ..., -0.41243485,\n",
       "         0.6598717 , -0.029063  ],\n",
       "       [ 0.07487375,  0.62892705, -0.1277031 , ..., -0.20496681,\n",
       "         0.44621164,  0.35458577],\n",
       "       ...,\n",
       "       [ 0.00215415,  0.03963464, -0.02555776, ..., -0.23670602,\n",
       "        -0.03042183,  0.27644476],\n",
       "       [-0.08850451,  0.41939262, -0.14636844, ..., -0.63478065,\n",
       "         0.12714297,  0.39889377],\n",
       "       [-0.00421663, -0.09909241,  0.34751567, ..., -0.12137473,\n",
       "         0.448606  ,  0.10963394]], shape=(14639, 768), dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_Encoded_Text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14639, 768)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means using personal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(x1, x2):\n",
    "    '''\n",
    "    x1: numpy array, shape = [D]\n",
    "    x2: numpy array, shape = [D]\n",
    "    RETURN\n",
    "        dist: float value\n",
    "    '''\n",
    "    #dist = np.sqrt(np.dot(x1-x2,x1-x2))\n",
    "    x1_length = np.dot(x1, x1)\n",
    "    x2_length = np.dot(x2, x2)\n",
    "    cosine = np.dot(x1, x2)/(x1_length*x2_length)\n",
    "    dist = np.arccos(cosine)/np.arccos(-1)\n",
    "\n",
    "    ## end\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestCentroid(input_vector, centroid_dict):\n",
    "    '''\n",
    "    coordinates_x: numpy array, shape = [D]\n",
    "    coordinates_centroid: dictionary, key = int, value = numpy array of shape [D]\n",
    "    RETURN\n",
    "        closest_centroid: int value\n",
    "    '''\n",
    "    closest_centroid = None\n",
    "\n",
    "    ## start your code here\n",
    "    smallest_dist = 10**10\n",
    "\n",
    "    for key in centroid_dict:\n",
    "        curr_centroid_vector = centroid_dict[key]\n",
    "        curr_dist = cos_distance(curr_centroid_vector, input_vector)\n",
    "        \n",
    "        if (curr_dist < smallest_dist):\n",
    "            closest_centroid = key\n",
    "            smallest_dist = curr_dist\n",
    "    \n",
    "    ## end\n",
    "    return closest_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using arccos(cosine_similarity)/π as the distance metric, the standard arithmetic mean used in k-means won’t work correctly because it doesn’t preserve unit directionality. \n",
    "\n",
    "Instead, we need to compute the centroid in a way that maintains the directional nature of the embeddings. \n",
    "\n",
    "The best approach is to use the normalized mean vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_centroid(cluster_vectors):\n",
    "    if len(cluster_vectors) == 0:\n",
    "        return None  # Handle empty clusters\n",
    "    \n",
    "    cluster_vectors = [vec / np.linalg.norm(vec) for vec in cluster_vectors] \n",
    "    \n",
    "    mean_vector = np.mean(cluster_vectors, axis=0).reshape(-1)  # Step 1: Compute the mean vector\n",
    "    norm = np.linalg.norm(mean_vector)  # Step 2: Compute its norm\n",
    "    \n",
    "    if norm == 0:\n",
    "        return np.zeros_like(mean_vector)  # Edge case: if the norm is 0, return a zero vector\n",
    "    \n",
    "    return mean_vector / norm  # Step 3: Normalize to get unit vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansClustering(X, index_centroids, k, n):\n",
    "    '''\n",
    "    X: numpy array, shape = [N, D]\n",
    "    index_centroids: list, shape = k\n",
    "    k: int value\n",
    "    n: int value\n",
    "    RETURN\n",
    "        repartition: dictionary, key = int, value = numpy array of shape [number of points in cluster, D]\n",
    "        coordinates: dictionary, key = int, value = numpy array of shape [D]\n",
    "    '''\n",
    "    repartition, centroids,  = None, dict()\n",
    "    ## start your code here\n",
    "    # Initialise your first centroids\n",
    "    for i in range(k):\n",
    "        centroids[i] = X[index_centroids[i]]\n",
    "        \n",
    "    # Define stopping criterion\n",
    "    for i in range(n):\n",
    "        # Initialise new dictionaries for repartition and coordinates\n",
    "        repartition = dict()\n",
    "        for i in range(k):\n",
    "            repartition[i] = []\n",
    "            \n",
    "        # Assign all the points to the closest cluster centroid\n",
    "        for vector in X:\n",
    "            repartition[closestCentroid(vector, centroids)].append(vector)\n",
    "\n",
    "        # Recompute the new centroids of the newly formed clusters\n",
    "        for cluster_key in repartition:\n",
    "            curr_cluster = repartition[cluster_key]\n",
    "            new_centroid = compute_new_centroid(curr_cluster)\n",
    "            centroids[cluster_key] = new_centroid\n",
    "        \n",
    "    \n",
    "    ## end\n",
    "    return repartition, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnKmeans(X, k, m):\n",
    "    '''\n",
    "    X: numpy array, shape = [N, D]\n",
    "    k: int value\n",
    "    m: int value\n",
    "    RETURN\n",
    "        position: numpy array, shape = [N]\n",
    "        centers: numpy array, shape = [k, D]\n",
    "    '''\n",
    "    position, centers = None, None\n",
    "    ## start your code here\n",
    "    kmeans = KMeans(n_clusters=k,n_init=1, max_iter=m).fit(X)\n",
    "    position, centers = kmeans.predict(X), kmeans.cluster_centers_\n",
    "    ## end\n",
    "    return position, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDist(x1, x2):\n",
    "    '''\n",
    "    x1: numpy array, shape = [D]\n",
    "    x2: numpy array, shape = [D]\n",
    "    RETURN\n",
    "        dist: float value\n",
    "    '''\n",
    "    dist = np.sqrt(np.dot(x1-x2,x1-x2))\n",
    "    ## start your code here\n",
    "    \n",
    "    \n",
    "    ## end\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial attempt using SK-Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, centers = sklearnKmeans(data, 3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters[position[index]].append(data[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_Centroid(clusters, centroids):\n",
    "\n",
    "    cluster_representatives = dict()\n",
    "    for cluster in clusters.keys():\n",
    "        cluster_representatives[cluster] = []\n",
    "\n",
    "    for cluster in clusters.keys():\n",
    "        curr_centroid = centroids[cluster]\n",
    "        smallest_dist = 10**10\n",
    "        closest_datapoint = clusters[cluster][0]\n",
    "\n",
    "        for vector in clusters[cluster]:\n",
    "            curr_dist = euclideanDist(vector, curr_centroid)\n",
    "            if (curr_dist < smallest_dist):\n",
    "                closest_datapoint = vector\n",
    "                smallest_dist = curr_dist\n",
    "        \n",
    "        cluster_representatives[cluster] = closest_datapoint\n",
    "        \n",
    "    ## end\n",
    "\n",
    "    return cluster_representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_representatives = closest_to_Centroid(clusters, centers)\n",
    "cluster_rep_indexes = [np.where(data == rep)[0][0] for rep in cluster_representatives.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes])[cluster] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'positive', 2: 'neutral'}"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = list(map(lambda x: labels[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 39.9%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments)\n",
    "print(f\"accuracy is {np.round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in first attempt is ≈ 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt using own K-Means Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_centroids = [np.array([random.randint(0, len(data) - 1)]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,768) and (1,768) not aligned: 768 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clusters2, centers \u001b[38;5;241m=\u001b[39m \u001b[43mKMeansClustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_centroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 26\u001b[0m, in \u001b[0;36mKMeansClustering\u001b[1;34m(X, index_centroids, k, n)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Assign all the points to the closest cluster centroid\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m---> 26\u001b[0m     repartition[\u001b[43mclosestCentroid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(vector)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Recompute the new centroids of the newly formed clusters\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_key \u001b[38;5;129;01min\u001b[39;00m repartition:\n",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m, in \u001b[0;36mclosestCentroid\u001b[1;34m(input_vector, centroid_dict)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m centroid_dict:\n\u001b[0;32m     14\u001b[0m     curr_centroid_vector \u001b[38;5;241m=\u001b[39m centroid_dict[key]\n\u001b[1;32m---> 15\u001b[0m     curr_dist \u001b[38;5;241m=\u001b[39m \u001b[43mcos_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_centroid_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (curr_dist \u001b[38;5;241m<\u001b[39m smallest_dist):\n\u001b[0;32m     18\u001b[0m         closest_centroid \u001b[38;5;241m=\u001b[39m key\n",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m, in \u001b[0;36mcos_distance\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mx1: numpy array, shape = [D]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mx2: numpy array, shape = [D]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mRETURN\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    dist: float value\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#dist = np.sqrt(np.dot(x1-x2,x1-x2))\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m x1_length \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m x2_length \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x2, x2)\n\u001b[0;32m     11\u001b[0m cosine \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x1, x2)\u001b[38;5;241m/\u001b[39m(x1_length\u001b[38;5;241m*\u001b[39mx2_length)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,768) and (1,768) not aligned: 768 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "clusters2, centers = KMeansClustering(data, index_centroids, 3, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
