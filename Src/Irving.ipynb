{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT1244 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import heapq as heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import random as random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bert Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Bert Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode sentences\n",
    "sentence1 = \"This is an example sentence with change negative.\"\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "\n",
    "sentence2 = \"This is an example sentence with change upset.\"\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "sentence3 = \"This is an example sentence with change happy.\"\n",
    "inputs3 = tokenizer(sentence3, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "    outputs3 = model(**inputs3)\n",
    "\n",
    "# Extract [CLS] token embedding (sentence-level representation)\n",
    "Vector_X = outputs1.last_hidden_state[:, 0, :]\n",
    "Vector_Y = outputs2.last_hidden_state[:, 0, :]\n",
    "Vector_Z = outputs3.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9878990054130554\n",
      "0.982489287853241\n"
     ]
    }
   ],
   "source": [
    "print(float(cosine_similarity(Vector_X, Vector_Y)[0][0]))\n",
    "print(float(cosine_similarity(Vector_X, Vector_Z)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    return output.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the closer concepts are have higher cosine similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  sentiment_confidence  \\\n",
       "0           neutral                1.0000   \n",
       "1          positive                0.3486   \n",
       "2           neutral                0.6837   \n",
       "3          negative                1.0000   \n",
       "4          negative                1.0000   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"../Data/Raw/Tweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14639, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica plus you've added commercials to the experience... tacky.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"text\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_filter(text):\n",
    "    return re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "def alpha_filter(text):\n",
    "    return re.sub(r'[^A-Za-z ]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: at_filter(x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: alpha_filter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>What  said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>thank you we got on a different flight to Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>leaving over  minutes Late Flight No warnings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Please bring American Airlines to BlackBerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>you have my money you change my flight and do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>we have  ppl so we need  know how many seats ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14639 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment  sentiment_confidence  \\\n",
       "0               neutral                1.0000   \n",
       "1              positive                0.3486   \n",
       "2               neutral                0.6837   \n",
       "3              negative                1.0000   \n",
       "4              negative                1.0000   \n",
       "...                 ...                   ...   \n",
       "14634          positive                0.3487   \n",
       "14635          negative                1.0000   \n",
       "14636           neutral                1.0000   \n",
       "14637          negative                1.0000   \n",
       "14638           neutral                0.6771   \n",
       "\n",
       "                                                    text  \n",
       "0                                             What  said  \n",
       "1       plus youve added commercials to the experienc...  \n",
       "2       I didnt today Must mean I need to take anothe...  \n",
       "3       its really aggressive to blast obnoxious ente...  \n",
       "4                and its a really big bad thing about it  \n",
       "...                                                  ...  \n",
       "14634   thank you we got on a different flight to Chi...  \n",
       "14635   leaving over  minutes Late Flight No warnings...  \n",
       "14636       Please bring American Airlines to BlackBerry  \n",
       "14637   you have my money you change my flight and do...  \n",
       "14638   we have  ppl so we need  know how many seats ...  \n",
       "\n",
       "[14639 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6269553931279459"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tweets[\"airline_sentiment\"] == \"negative\")/len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"../Data/Cleaned/CleanTweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Tweets into Vectors in R^n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = tweets[\"text\"].apply(lambda x: bert_encode(x))\n",
    "data = np.array(list(map(lambda x: x[0], sentence_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Data as a CSV so don't need to re-encode every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_frame = pd.DataFrame(data)\n",
    "#data_frame.to_txt(\"../Data/Cleaned/BERT_Vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pq.write_table(table, \"../Data/Cleaned/BERT_Vectors.oarquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Encodings can be found in data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_Encoded_Text = pd.read_parquet(\"../Data/Cleaned/BERT_Vectors.oarquet\")\n",
    "data = Bert_Encoded_Text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_Encoded_Text_raw = pd.read_parquet(\"../Data/Cleaned/TWITTERBERT_Vectors1.parquet\")\n",
    "data_unclean = Bert_Encoded_Text_raw.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14639, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unclean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means using personal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(x1, x2):\n",
    "    '''\n",
    "    x1: numpy array, shape = [D]\n",
    "    x2: numpy array, shape = [D]\n",
    "    RETURN\n",
    "        dist: float value\n",
    "    '''\n",
    "    #dist = np.sqrt(np.dot(x1-x2,x1-x2))\n",
    "    x1_length = np.dot(x1, x1)\n",
    "    x2_length = np.dot(x2, x2)\n",
    "    cosine = np.dot(x1, x2)/(x1_length*x2_length)\n",
    "    dist = np.arccos(cosine)/np.arccos(-1)\n",
    "\n",
    "    ## end\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestCentroid(input_vector, centroid_dict, distance):\n",
    "    '''\n",
    "    coordinates_x: numpy array, shape = [D]\n",
    "    coordinates_centroid: dictionary, key = int, value = numpy array of shape [D]\n",
    "    RETURN\n",
    "        closest_centroid: int value\n",
    "    '''\n",
    "    closest_centroid = None\n",
    "\n",
    "    ## start your code here\n",
    "    smallest_dist = 10**10\n",
    "\n",
    "    for key in centroid_dict:\n",
    "        curr_centroid_vector = centroid_dict[key]\n",
    "        curr_centroid_vector = curr_centroid_vector.reshape(-1)\n",
    "        curr_dist = distance(curr_centroid_vector, input_vector)\n",
    "        \n",
    "        if (curr_dist < smallest_dist):\n",
    "            closest_centroid = key\n",
    "            smallest_dist = curr_dist\n",
    "    \n",
    "    ## end\n",
    "    return closest_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using arccos(cosine_similarity)/π as the distance metric, the standard arithmetic mean used in k-means won’t work correctly because it doesn’t preserve unit directionality. \n",
    "\n",
    "Instead, we need to compute the centroid in a way that maintains the directional nature of the embeddings. \n",
    "\n",
    "The best approach is to use the normalized mean vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_centroid(cluster_vectors):\n",
    "    if len(cluster_vectors) == 0:\n",
    "        return None  # Handle empty clusters\n",
    "    \n",
    "    cluster_vectors = [vec / np.linalg.norm(vec) for vec in cluster_vectors] \n",
    "    \n",
    "    mean_vector = np.mean(cluster_vectors, axis=0).reshape(-1)  # Step 1: Compute the mean vector\n",
    "    norm = np.linalg.norm(mean_vector)  # Step 2: Compute its norm\n",
    "    \n",
    "    if norm == 0:\n",
    "        return np.zeros_like(mean_vector)  # Edge case: if the norm is 0, return a zero vector\n",
    "    \n",
    "    return mean_vector / norm  # Step 3: Normalize to get unit vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansClustering(X, initial_centroids, distance, k, n, tolerance=1e-20):\n",
    "    '''\n",
    "    X: numpy array, shape = [N, D]\n",
    "    index_centroids: list, shape = k\n",
    "    k: int value\n",
    "    n: int value\n",
    "    RETURN\n",
    "        repartition: dictionary, key = int, value = numpy array of shape [number of points in cluster, D]\n",
    "        coordinates: dictionary, key = int, value = numpy array of shape [D]\n",
    "    '''\n",
    "\n",
    "\n",
    "    repartition, centroids,  = None, dict()\n",
    "    ## start your code here\n",
    "    # Initialise your first centroids\n",
    "    for i in range(k):\n",
    "        centroids[i] = initial_centroids[i]\n",
    "        \n",
    "    # Define stopping criterion\n",
    "    for i in range(n):\n",
    "        # Initialise new dictionaries for repartition and coordinates\n",
    "        repartition = dict()\n",
    "        for i in range(k):\n",
    "            repartition[i] = []\n",
    "            \n",
    "        # Assign all the points to the closest cluster centroid\n",
    "        for vector in X:\n",
    "            repartition[closestCentroid(vector, centroids, distance)].append(vector)\n",
    "\n",
    "        \n",
    "        # Compute new centroids\n",
    "        new_centroids = {}\n",
    "        for cluster_key in repartition:\n",
    "            new_centroids[cluster_key] = compute_new_centroid(repartition[cluster_key])\n",
    "        \n",
    "        # Check for early stopping (convergence)\n",
    "        centroid_changes = [np.linalg.norm(new_centroids[cluster_key] - centroids[cluster_key]) for cluster_key in centroids]\n",
    "        max_change = max(centroid_changes)\n",
    "\n",
    "        # If the max change in centroids is smaller than the tolerance, stop early\n",
    "        if max_change < tolerance:\n",
    "            print(f\"Converged early after {i+1} iterations.\")\n",
    "            return repartition, centroids\n",
    "        \n",
    "        # Update centroids for the next iteration\n",
    "        centroids = new_centroids\n",
    "        \n",
    "    \n",
    "    ## end\n",
    "    return repartition, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnKmeans(X, k, m):\n",
    "    '''\n",
    "    X: numpy array, shape = [N, D]\n",
    "    k: int value\n",
    "    m: int value\n",
    "    RETURN\n",
    "        position: numpy array, shape = [N]\n",
    "        centers: numpy array, shape = [k, D]\n",
    "    '''\n",
    "    position, centers = None, None\n",
    "    ## start your code here\n",
    "    kmeans = KMeans(n_clusters=k, init=\"random\", n_init=1, max_iter=m).fit(X)\n",
    "    position, centers = kmeans.predict(X), kmeans.cluster_centers_\n",
    "    ## end\n",
    "    return position, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearnKmeans_plus(X, k, m):\n",
    "    '''\n",
    "    X: numpy array, shape = [N, D]\n",
    "    k: int value\n",
    "    m: int value\n",
    "    RETURN\n",
    "        position: numpy array, shape = [N]\n",
    "        centers: numpy array, shape = [k, D]\n",
    "    '''\n",
    "    position, centers = None, None\n",
    "    ## start your code here\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", n_init=1, max_iter=m).fit(X)\n",
    "    position, centers = kmeans.predict(X), kmeans.cluster_centers_\n",
    "    ## end\n",
    "    return position, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDist(x1, x2):\n",
    "    '''\n",
    "    x1: numpy array, shape = [D]\n",
    "    x2: numpy array, shape = [D]\n",
    "    RETURN\n",
    "        dist: float value\n",
    "    '''\n",
    "    dist = np.sqrt(np.dot(x1-x2,x1-x2))\n",
    "    ## start your code here\n",
    "    \n",
    "    \n",
    "    ## end\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_Centroid(clusters, centroids, distance):\n",
    "\n",
    "    cluster_representatives = dict()\n",
    "    for cluster in clusters.keys():\n",
    "        cluster_representatives[cluster] = []\n",
    "\n",
    "    for cluster in clusters.keys():\n",
    "        curr_centroid = centroids[cluster]\n",
    "        smallest_dist = 10**10\n",
    "        closest_datapoint = clusters[cluster][0]\n",
    "\n",
    "        for vector in clusters[cluster]:\n",
    "            curr_dist = distance(vector, curr_centroid)\n",
    "            if (curr_dist < smallest_dist):\n",
    "                closest_datapoint = vector\n",
    "                smallest_dist = curr_dist\n",
    "        \n",
    "        cluster_representatives[cluster] = closest_datapoint\n",
    "        \n",
    "    ## end\n",
    "\n",
    "    return cluster_representatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial attempt using SK-Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, centers = sklearnKmeans(data, 3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 39.7%\n"
     ]
    }
   ],
   "source": [
    "clusters = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters[position[index]].append(data[index])\n",
    "\n",
    "cluster_representatives = closest_to_Centroid(clusters, centers, euclideanDist)\n",
    "cluster_rep_indexes = [np.where(data == rep)[0][0] for rep in cluster_representatives.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes])[cluster] \n",
    "\n",
    "predicted_sentiments = list(map(lambda x: labels[x], position))\n",
    "\n",
    "accuracy = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments)\n",
    "print(f\"accuracy is {np.round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in First attempt is ≈ 39.70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt using SK-Learn Library (K-Means++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, centers = sklearnKmeans_plus(data, 3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 40.08%\n"
     ]
    }
   ],
   "source": [
    "clusters = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters[position[index]].append(data[index])\n",
    "\n",
    "cluster_representatives = closest_to_Centroid(clusters, centers, euclideanDist)\n",
    "cluster_rep_indexes = [np.where(data == rep)[0][0] for rep in cluster_representatives.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes])[cluster] \n",
    "\n",
    "predicted_sentiments = list(map(lambda x: labels[x], position))\n",
    "\n",
    "accuracy = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments)\n",
    "print(f\"accuracy is {np.round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in Second attempt is ≈ 40.08%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Attempt using own K-Means Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged early after 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "initial_centroids = [np.array(data[random.randint(0, len(data) - 1)]) for i in range(3)]\n",
    "clusters2, centers2 = KMeansClustering(data, initial_centroids, cos_distance, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters2[position[index]].append(data[index])\n",
    "\n",
    "cluster_representatives2 = closest_to_Centroid(clusters2, centers2, cos_distance)\n",
    "cluster_rep_indexes2 = [np.where(data == rep)[0][0] for rep in cluster_representatives2.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels2 = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels2[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes2])[cluster] \n",
    "\n",
    "predicted_sentiments2 = list(map(lambda x: labels2[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 48.23%\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments2)\n",
    "print(f\"accuracy is {np.round(accuracy2*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in Third attempt is ≈ 56.07%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Attempt using own K-Means++ Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_initalizer(data, k, distance_func):\n",
    "    \n",
    "    n_samples = data.shape[0]\n",
    "    # Step 1: Choose the first centroid randomly\n",
    "    first_index = random.randint(0, n_samples - 1)\n",
    "    centroids = [data[first_index]]\n",
    "    \n",
    "    for _ in range(1, k):\n",
    "        # Compute the minimum distance to any chosen centroid\n",
    "        distances = np.array([min(distance_func(x, c) for c in centroids) for x in data])\n",
    "        \n",
    "        # Normalize distances to form a probability distribution\n",
    "        probabilities = distances / distances.sum()\n",
    "        \n",
    "        # Select next centroid based on weighted probability distribution\n",
    "        next_index = np.random.choice(n_samples, p=probabilities)\n",
    "        centroids.append(data[next_index])\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged early after 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "initial_centroids = centroid_initalizer(data, 3, cos_distance)\n",
    "clusters2, centers2 = KMeansClustering(data, initial_centroids, cos_distance, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters2[position[index]].append(data[index])\n",
    "\n",
    "cluster_representatives2 = closest_to_Centroid(clusters2, centers2, cos_distance)\n",
    "cluster_rep_indexes2 = [np.where(data == rep)[0][0] for rep in cluster_representatives2.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels2 = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels2[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes2])[cluster] \n",
    "\n",
    "predicted_sentiments2 = list(map(lambda x: labels2[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 39.28%\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments2)\n",
    "print(f\"accuracy is {np.round(accuracy2*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in Fourth attempt is ≈ 56.07%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged early after 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "initial_centroids = centroid_initalizer(data_unclean, 3, cos_distance)\n",
    "clusters2, centers2 = KMeansClustering(data_unclean, initial_centroids, cos_distance, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters2[position[index]].append(data_unclean[index])\n",
    "\n",
    "cluster_representatives2 = closest_to_Centroid(clusters2, centers2, cos_distance)\n",
    "cluster_rep_indexes2 = [np.where(data_unclean == rep)[0][0] for rep in cluster_representatives2.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels2 = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels2[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes2])[cluster] \n",
    "\n",
    "predicted_sentiments2 = list(map(lambda x: labels2[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 56.07%\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments2)\n",
    "print(f\"accuracy is {np.round(accuracy2*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think of new ways to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, centers = sklearnKmeans_plus(data_unclean, 3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 62.7%\n"
     ]
    }
   ],
   "source": [
    "clusters = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters[position[index]].append(data_unclean[index])\n",
    "\n",
    "cluster_representatives = closest_to_Centroid(clusters, centers, euclideanDist)\n",
    "cluster_rep_indexes = [np.where(data_unclean == rep)[0][0] for rep in cluster_representatives.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes])[cluster] \n",
    "\n",
    "predicted_sentiments = list(map(lambda x: labels[x], position))\n",
    "\n",
    "accuracy = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments)\n",
    "print(f\"accuracy is {np.round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "bert_vectors_std = scaler.fit_transform(Bert_Encoded_Text)  # Shape: (N, 768)\n",
    "\n",
    "# Choose the number of components\n",
    "n_components = 50 # Example: Reduce to 50 dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "bert_pca = pca.fit_transform(bert_vectors_std)  # Shape: (N, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged early after 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "initial_centroids = centroid_initalizer(bert_pca, 3, cos_distance)\n",
    "clusters2, centers2 = KMeansClustering(bert_pca, initial_centroids, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters2[position[index]].append(bert_pca[index])\n",
    "\n",
    "cluster_representatives2 = closest_to_Centroid(clusters2, centers2, cos_distance)\n",
    "cluster_rep_indexes2 = [np.where(bert_pca == rep)[0][0] for rep in cluster_representatives2.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels2 = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels2[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes2])[cluster] \n",
    "\n",
    "predicted_sentiments2 = list(map(lambda x: labels2[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 35.64%\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments2)\n",
    "print(f\"accuracy is {np.round(accuracy2*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in Sixth attempt is ≈ 35.64%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Might Not Be the Best Approach\n",
    "#### PCA finds linear patterns, but BERT vectors often have nonlinear structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP (Uniform Manifold Approximation and Projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP with K-Means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = um.UMAP(n_components=50, random_state=42)\n",
    "bert_umap = umap.fit_transform(Bert_Encoded_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged early after 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "initial_centroids = centroid_initalizer(bert_umap, 3, cos_distance)\n",
    "clusters2, centers2 = KMeansClustering(bert_umap, initial_centroids, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = {0:[], 1:[], 2:[]}\n",
    "N = len(position)\n",
    "\n",
    "for index in range(N):\n",
    "    clusters2[position[index]].append(bert_umap[index])\n",
    "\n",
    "cluster_representatives2 = closest_to_Centroid(clusters2, centers2, cos_distance)\n",
    "cluster_rep_indexes2 = [np.where(bert_umap == rep)[0][0] for rep in cluster_representatives2.values()]\n",
    "\n",
    "# labelling the centroids to test accuracy\n",
    "possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "possible_clusters = [0, 1, 2]\n",
    "labels2 = dict()\n",
    "\n",
    "for cluster in possible_clusters:\n",
    "    labels2[cluster] = list(tweets[\"airline_sentiment\"][cluster_rep_indexes2])[cluster] \n",
    "\n",
    "predicted_sentiments2 = list(map(lambda x: labels2[x], position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 54.55%\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = np.mean(tweets[\"airline_sentiment\"] ==  predicted_sentiments2)\n",
    "print(f\"accuracy is {np.round(accuracy2*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy achieved in Seventh attempt is ≈ 54.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It seems like BERT Encoding with Cosine-distance K-Means++ still outperforms it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, metric='cosine')\n",
    "\n",
    "# Fit the model\n",
    "clusterer.fit(Bert_Encoded_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusterer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mclusterer\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m clusterer\u001b[38;5;241m.\u001b[39mprobabilities_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clusterer' is not defined"
     ]
    }
   ],
   "source": [
    "labels = clusterer.labels_\n",
    "print(f\"Cluster Labels: {labels}\")\n",
    "\n",
    "probabilities = clusterer.probabilities_\n",
    "print(f\"Cluster Probabilities: {probabilities}\")\n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"Number of clusters: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I should keep a table of all the different models with their accuracy and runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new approach create 3 vectors \"negative\", \"positive\", \"neutral\"\n",
    "\n",
    "sentiment_as_text = list(map(bert_encode, [\"negative bad unhappy\", \"positive good happy\", \"happy\"]))\n",
    "basis_vectors = np.array(list(map(lambda x: x[0], sentiment_as_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_hashable(arr):\n",
    "    return tuple(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new approach create 3 vectors \"negative\", \"positive\", \"neutral\"\n",
    "\n",
    "sentiment_as_text = list(map(bert_encode, [\"negative\", \"positive\", \"neutral\"]))\n",
    "basis_vectors = np.array(list(map(lambda x: x[0], sentiment_as_text)))\n",
    "\n",
    "\n",
    "pseudo_clusters = {\"negative\":[], \"positive\":[], \"neutral\":[]}\n",
    "e1,e2,e3 = basis_vectors\n",
    "e1,e2,e3 = tuple(e1), tuple(e2), tuple(e3)\n",
    "basis_vector_labels = {e1: \"negative\", e2: \"positive\", e3: \"neutral\"}\n",
    "\n",
    "for tweet_vector in data:\n",
    "    curr_closet_basis_vector = 0\n",
    "    curr_smallest_cos_dist = 1\n",
    "    for basis_vector in basis_vectors:\n",
    "        distance = cos_distance(tweet_vector, basis_vector)\n",
    "        if (distance < curr_smallest_cos_dist):\n",
    "            curr_closet_basis_vector = basis_vector\n",
    "\n",
    "    pseudo_clusters[basis_vector_labels[make_it_hashable(curr_closet_basis_vector)]].append(tweet_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example representative sentences for each sentiment\n",
    "positive_sentences = [\"Yay I am happy!\", \"This makes me pleased.\", \"Good job airline!\"]\n",
    "negative_sentences = [\"This is terrible!\", \"I hate this.\", \"Awful experience!\"]\n",
    "neutral_sentences = [\"It's okay.\", \"Not bad, not great.\", \"It was fine.\"]\n",
    "\n",
    "# Get BERT embeddings\n",
    "positive_vectors = np.array([bert_encode(s)[0] for s in positive_sentences])\n",
    "negative_vectors = np.array([bert_encode(s)[0] for s in negative_sentences])\n",
    "neutral_vectors = np.array([bert_encode(s)[0] for s in neutral_sentences])\n",
    "\n",
    "# Compute a basis for each sentiment by taking the row span\n",
    "positive_basis = np.linalg.qr(positive_vectors.T)[0]  # QR decomposition to orthogonalize\n",
    "negative_basis = np.linalg.qr(negative_vectors.T)[0]\n",
    "neutral_basis = np.linalg.qr(neutral_vectors.T)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_basis(vector, basis):\n",
    "    \"\"\"Projects vector onto the space spanned by basis.\"\"\"\n",
    "    return basis @ (basis.T @ vector)\n",
    "\n",
    "def classify_tweet(tweet_vector):\n",
    "    \"\"\"Assign tweet vector to the closest hyperplane using cosine similarity.\"\"\"\n",
    "    proj_positive = project_onto_basis(tweet_vector, positive_basis)\n",
    "    proj_negative = project_onto_basis(tweet_vector, negative_basis)\n",
    "    proj_neutral = project_onto_basis(tweet_vector, neutral_basis)\n",
    "\n",
    "    # Compute cosine similarity with original tweet vector\n",
    "    cos_sim_positive = np.dot(proj_positive, tweet_vector) / (np.linalg.norm(proj_positive) * np.linalg.norm(tweet_vector))\n",
    "    cos_sim_negative = np.dot(proj_negative, tweet_vector) / (np.linalg.norm(proj_negative) * np.linalg.norm(tweet_vector))\n",
    "    cos_sim_neutral = np.dot(proj_neutral, tweet_vector) / (np.linalg.norm(proj_neutral) * np.linalg.norm(tweet_vector))\n",
    "\n",
    "    similarities = {\"positive\": cos_sim_positive, \"negative\": cos_sim_negative, \"neutral\": cos_sim_neutral}\n",
    "\n",
    "    return max(similarities, key=similarities.get)  # Assign sentiment with highest similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_clusters = {\"positive\": [], \"negative\": [], \"neutral\": []}\n",
    "\n",
    "for tweet_vector in data:\n",
    "    sentiment = classify_tweet(tweet_vector)\n",
    "    pseudo_clusters[sentiment].append(tweet_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pseudo_clusters['neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet_soft(tweet_vector):\n",
    "    proj_positive = project_onto_basis(tweet_vector, positive_basis)\n",
    "    proj_negative = project_onto_basis(tweet_vector, negative_basis)\n",
    "    proj_neutral = project_onto_basis(tweet_vector, neutral_basis)\n",
    "\n",
    "    cos_sim_positive = np.dot(proj_positive, tweet_vector) / (np.linalg.norm(proj_positive) * np.linalg.norm(tweet_vector))\n",
    "    cos_sim_negative = np.dot(proj_negative, tweet_vector) / (np.linalg.norm(proj_negative) * np.linalg.norm(tweet_vector))\n",
    "    cos_sim_neutral = np.dot(proj_neutral, tweet_vector) / (np.linalg.norm(proj_neutral) * np.linalg.norm(tweet_vector))\n",
    "\n",
    "    scores = np.array([cos_sim_positive, cos_sim_negative, cos_sim_neutral])\n",
    "    softmax_scores = np.exp(scores) / np.sum(np.exp(scores))  # Softmax for probabilities\n",
    "    sentiment = np.random.choice([\"positive\", \"negative\", \"neutral\"], p = softmax_scores)\n",
    "\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_clusters = {\"positive\": [], \"negative\": [], \"neutral\": []}\n",
    "\n",
    "for tweet_vector in data:\n",
    "    sentiment = classify_tweet_soft(tweet_vector)\n",
    "    pseudo_clusters[sentiment].append(tweet_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5014, 4773, 4852)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(pseudo_clusters['positive']), len(pseudo_clusters['negative']), len(pseudo_clusters['neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2362, 9178, 3099)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(tweets[\"airline_sentiment\"] == 'positive'), sum(tweets[\"airline_sentiment\"] == 'negative'), sum(tweets[\"airline_sentiment\"] == 'neutral'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
